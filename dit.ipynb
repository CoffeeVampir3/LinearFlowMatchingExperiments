{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd7c834-a7e1-4e75-8f1e-d3da68c3c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minimaldit.minimaldit import MinimalDiT\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from schedulefree import AdamWScheduleFree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b30807-a3fc-4aef-b570-681ce1ac4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preload_dataset(image_size=256, device=\"cuda\"):\n",
    "    \"\"\"Preload and cache the entire dataset in GPU memory\"\"\"\n",
    "    print(\"Loading and preprocessing dataset...\")\n",
    "    dataset = load_dataset(\"reach-vb/pokemon-blip-captions\", split=\"train\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((image_size, image_size), antialias=True),\n",
    "        transforms.Lambda(lambda x: (x * 2) - 1)  # Scale to [-1, 1]\n",
    "    ])\n",
    "\n",
    "    # Process all images at once\n",
    "    all_images = []\n",
    "    for example in dataset:\n",
    "        # The Pokemon dataset stores images in 'image' field\n",
    "        img_tensor = transform(example['image'].convert('RGB'))\n",
    "        all_images.append(img_tensor)\n",
    "\n",
    "    # Stack all images into a single tensor and move to GPU\n",
    "    images_tensor = torch.stack(all_images).to(device)\n",
    "    print(f\"Dataset loaded: {images_tensor.shape} ({images_tensor.element_size() * images_tensor.nelement() / 1024/1024:.2f} MB)\")\n",
    "    \n",
    "    return TensorDataset(images_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c61c0b-d896-4e8f-83ec-2418a492d0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing dataset...\n",
      "Dataset loaded: torch.Size([833, 3, 128, 128]) (156.19 MB)\n"
     ]
    }
   ],
   "source": [
    "# Preload dataset to GPU\n",
    "img_size = 128\n",
    "batch_size=32\n",
    "device=\"cuda:0\"\n",
    "dataset = preload_dataset(image_size=img_size, device=device)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d3f99-8434-4778-ab9b-c8fb9f674fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Loss: 1.9879\n",
      "Epoch 1, Average Loss: 1.7572\n",
      "Epoch 2, Average Loss: 1.5410\n",
      "Epoch 3, Average Loss: 1.4283\n",
      "Epoch 4, Average Loss: 1.3529\n",
      "Epoch 5, Average Loss: 1.3119\n",
      "Epoch 6, Average Loss: 1.2704\n",
      "Epoch 7, Average Loss: 1.2490\n",
      "Epoch 8, Average Loss: 1.2316\n",
      "Epoch 9, Average Loss: 1.2126\n",
      "Epoch 10, Average Loss: 1.1839\n",
      "Epoch 11, Average Loss: 1.1617\n",
      "Epoch 12, Average Loss: 1.1502\n",
      "Epoch 13, Average Loss: 1.1393\n",
      "Epoch 14, Average Loss: 1.1385\n",
      "Epoch 15, Average Loss: 1.1300\n",
      "Epoch 16, Average Loss: 1.1250\n",
      "Epoch 17, Average Loss: 1.1171\n",
      "Epoch 18, Average Loss: 1.1162\n",
      "Epoch 19, Average Loss: 1.1146\n",
      "Epoch 20, Average Loss: 1.1105\n",
      "Epoch 21, Average Loss: 1.1030\n",
      "Epoch 22, Average Loss: 1.0994\n",
      "Epoch 23, Average Loss: 1.0963\n",
      "Epoch 24, Average Loss: 1.0866\n",
      "Epoch 25, Average Loss: 1.0837\n",
      "Epoch 26, Average Loss: 1.0709\n",
      "Epoch 27, Average Loss: 1.0617\n",
      "Epoch 28, Average Loss: 1.0570\n",
      "Epoch 29, Average Loss: 1.0475\n",
      "Epoch 30, Average Loss: 1.0411\n",
      "Epoch 31, Average Loss: 1.0288\n",
      "Epoch 32, Average Loss: 1.0243\n",
      "Epoch 33, Average Loss: 1.0189\n",
      "Epoch 34, Average Loss: 1.0065\n",
      "Epoch 35, Average Loss: 0.9989\n",
      "Epoch 36, Average Loss: 1.0020\n",
      "Epoch 37, Average Loss: 1.0159\n",
      "Epoch 38, Average Loss: 0.9778\n",
      "Epoch 39, Average Loss: 0.9692\n",
      "Epoch 40, Average Loss: 0.9674\n",
      "Epoch 41, Average Loss: 0.9601\n",
      "Epoch 42, Average Loss: 0.9604\n",
      "Epoch 43, Average Loss: 0.9486\n",
      "Epoch 44, Average Loss: 0.9408\n",
      "Epoch 45, Average Loss: 0.9334\n",
      "Epoch 46, Average Loss: 0.9347\n",
      "Epoch 47, Average Loss: 0.9277\n",
      "Epoch 48, Average Loss: 0.9176\n",
      "Epoch 49, Average Loss: 0.9167\n",
      "Epoch 50, Average Loss: 0.9207\n",
      "Epoch 51, Average Loss: 0.9122\n",
      "Epoch 52, Average Loss: 0.9036\n",
      "Epoch 53, Average Loss: 0.9006\n",
      "Epoch 54, Average Loss: 0.8941\n",
      "Epoch 55, Average Loss: 0.8893\n",
      "Epoch 56, Average Loss: 0.8894\n",
      "Epoch 57, Average Loss: 0.8851\n",
      "Epoch 58, Average Loss: 0.8786\n",
      "Epoch 59, Average Loss: 0.8724\n",
      "Epoch 60, Average Loss: 0.8822\n",
      "Epoch 61, Average Loss: 0.8883\n",
      "Epoch 62, Average Loss: 0.8658\n",
      "Epoch 63, Average Loss: 0.8585\n",
      "Epoch 64, Average Loss: 0.8634\n",
      "Epoch 65, Average Loss: 0.8514\n",
      "Epoch 66, Average Loss: 0.8551\n",
      "Epoch 67, Average Loss: 0.8528\n",
      "Epoch 68, Average Loss: 0.8465\n",
      "Epoch 69, Average Loss: 0.8458\n",
      "Epoch 70, Average Loss: 0.8430\n",
      "Epoch 71, Average Loss: 0.8444\n",
      "Epoch 72, Average Loss: 0.8350\n",
      "Epoch 73, Average Loss: 0.8376\n",
      "Epoch 74, Average Loss: 0.8366\n",
      "Epoch 75, Average Loss: 0.8313\n",
      "Epoch 76, Average Loss: 0.8244\n",
      "Epoch 77, Average Loss: 0.8197\n",
      "Epoch 78, Average Loss: 0.8285\n",
      "Epoch 79, Average Loss: 0.8222\n",
      "Epoch 80, Average Loss: 0.8215\n",
      "Epoch 81, Average Loss: 0.8273\n",
      "Epoch 82, Average Loss: 0.8167\n",
      "Epoch 83, Average Loss: 0.8193\n",
      "Epoch 84, Average Loss: 0.8147\n",
      "Epoch 85, Average Loss: 0.8010\n",
      "Epoch 86, Average Loss: 0.8031\n",
      "Epoch 87, Average Loss: 0.8059\n",
      "Epoch 88, Average Loss: 0.8021\n",
      "Epoch 89, Average Loss: 0.7984\n",
      "Epoch 90, Average Loss: 0.8079\n",
      "Epoch 91, Average Loss: 0.7991\n",
      "Epoch 92, Average Loss: 0.7938\n",
      "Epoch 93, Average Loss: 0.7899\n",
      "Epoch 94, Average Loss: 0.7854\n",
      "Epoch 95, Average Loss: 0.7961\n",
      "Epoch 96, Average Loss: 0.7767\n",
      "Epoch 97, Average Loss: 0.7736\n",
      "Epoch 98, Average Loss: 0.7924\n",
      "Epoch 99, Average Loss: 0.7878\n",
      "Epoch 100, Average Loss: 0.7738\n",
      "Epoch 101, Average Loss: 0.7691\n",
      "Epoch 102, Average Loss: 0.7651\n",
      "Epoch 103, Average Loss: 0.7654\n",
      "Epoch 104, Average Loss: 0.7696\n",
      "Epoch 105, Average Loss: 0.7749\n",
      "Epoch 106, Average Loss: 0.7674\n",
      "Epoch 107, Average Loss: 0.7637\n",
      "Epoch 108, Average Loss: 0.7637\n",
      "Epoch 109, Average Loss: 0.7554\n",
      "Epoch 110, Average Loss: 0.7572\n",
      "Epoch 111, Average Loss: 0.7687\n",
      "Epoch 112, Average Loss: 0.7594\n",
      "Epoch 113, Average Loss: 0.7729\n",
      "Epoch 114, Average Loss: 0.7615\n",
      "Epoch 115, Average Loss: 0.7522\n",
      "Epoch 116, Average Loss: 0.7585\n",
      "Epoch 117, Average Loss: 0.7508\n",
      "Epoch 118, Average Loss: 0.7444\n",
      "Epoch 119, Average Loss: 0.7459\n",
      "Epoch 120, Average Loss: 0.7475\n",
      "Epoch 121, Average Loss: 0.7493\n",
      "Epoch 122, Average Loss: 0.7455\n",
      "Epoch 123, Average Loss: 0.7436\n",
      "Epoch 124, Average Loss: 0.7437\n",
      "Epoch 125, Average Loss: 0.7312\n",
      "Epoch 126, Average Loss: 0.7401\n",
      "Epoch 127, Average Loss: 0.7404\n",
      "Epoch 128, Average Loss: 0.7415\n",
      "Epoch 129, Average Loss: 0.7472\n",
      "Epoch 130, Average Loss: 0.7425\n",
      "Epoch 131, Average Loss: 0.7365\n",
      "Epoch 132, Average Loss: 0.7384\n",
      "Epoch 133, Average Loss: 0.7377\n",
      "Epoch 134, Average Loss: 0.7287\n",
      "Epoch 135, Average Loss: 0.7333\n",
      "Epoch 136, Average Loss: 0.7284\n",
      "Epoch 137, Average Loss: 0.7239\n",
      "Epoch 138, Average Loss: 0.7239\n",
      "Epoch 139, Average Loss: 0.7258\n",
      "Epoch 140, Average Loss: 0.7212\n",
      "Epoch 141, Average Loss: 0.7211\n",
      "Epoch 142, Average Loss: 0.7124\n",
      "Epoch 143, Average Loss: 0.7159\n",
      "Epoch 144, Average Loss: 0.7237\n",
      "Epoch 145, Average Loss: 0.7185\n",
      "Epoch 146, Average Loss: 0.7239\n",
      "Epoch 147, Average Loss: 0.7158\n",
      "Epoch 148, Average Loss: 0.7236\n",
      "Epoch 149, Average Loss: 0.7115\n",
      "Epoch 150, Average Loss: 0.7027\n",
      "Epoch 151, Average Loss: 0.7085\n",
      "Epoch 152, Average Loss: 0.7023\n",
      "Epoch 153, Average Loss: 0.7074\n",
      "Epoch 154, Average Loss: 0.7025\n",
      "Epoch 155, Average Loss: 0.6995\n",
      "Epoch 156, Average Loss: 0.7093\n",
      "Epoch 157, Average Loss: 0.6998\n",
      "Epoch 158, Average Loss: 0.7074\n",
      "Epoch 159, Average Loss: 0.7029\n",
      "Epoch 160, Average Loss: 0.7080\n",
      "Epoch 161, Average Loss: 0.7108\n",
      "Epoch 162, Average Loss: 0.6980\n",
      "Epoch 163, Average Loss: 0.7021\n",
      "Epoch 164, Average Loss: 0.7009\n",
      "Epoch 165, Average Loss: 0.6946\n",
      "Epoch 166, Average Loss: 0.6909\n",
      "Epoch 167, Average Loss: 0.7004\n",
      "Epoch 168, Average Loss: 0.6889\n",
      "Epoch 169, Average Loss: 0.7062\n",
      "Epoch 170, Average Loss: 0.7105\n",
      "Epoch 171, Average Loss: 0.6927\n",
      "Epoch 172, Average Loss: 0.6882\n",
      "Epoch 173, Average Loss: 0.6879\n",
      "Epoch 174, Average Loss: 0.6987\n",
      "Epoch 175, Average Loss: 0.6850\n",
      "Epoch 176, Average Loss: 0.6932\n",
      "Epoch 177, Average Loss: 0.6781\n",
      "Epoch 178, Average Loss: 0.6814\n",
      "Epoch 179, Average Loss: 0.6779\n",
      "Epoch 180, Average Loss: 0.6850\n",
      "Epoch 181, Average Loss: 0.6901\n",
      "Epoch 182, Average Loss: 0.6854\n",
      "Epoch 183, Average Loss: 0.6771\n",
      "Epoch 184, Average Loss: 0.6743\n",
      "Epoch 185, Average Loss: 0.6796\n",
      "Epoch 186, Average Loss: 0.6806\n",
      "Epoch 187, Average Loss: 0.6927\n",
      "Epoch 188, Average Loss: 0.6784\n",
      "Epoch 189, Average Loss: 0.6741\n",
      "Epoch 190, Average Loss: 0.6666\n",
      "Epoch 191, Average Loss: 0.6762\n",
      "Epoch 192, Average Loss: 0.6786\n",
      "Epoch 193, Average Loss: 0.6740\n",
      "Epoch 194, Average Loss: 0.6760\n",
      "Epoch 195, Average Loss: 0.6822\n",
      "Epoch 196, Average Loss: 0.6737\n",
      "Epoch 197, Average Loss: 0.6732\n",
      "Epoch 198, Average Loss: 0.6933\n",
      "Epoch 199, Average Loss: 0.6778\n",
      "Epoch 200, Average Loss: 0.6720\n",
      "Epoch 201, Average Loss: 0.6561\n",
      "Epoch 202, Average Loss: 0.6586\n",
      "Epoch 203, Average Loss: 0.6891\n",
      "Epoch 204, Average Loss: 0.6693\n",
      "Epoch 205, Average Loss: 0.6598\n",
      "Epoch 206, Average Loss: 0.6659\n",
      "Epoch 207, Average Loss: 0.6557\n",
      "Epoch 208, Average Loss: 0.6717\n",
      "Epoch 209, Average Loss: 0.6883\n",
      "Epoch 210, Average Loss: 0.6635\n",
      "Epoch 211, Average Loss: 0.6597\n",
      "Epoch 212, Average Loss: 0.6690\n",
      "Epoch 213, Average Loss: 0.6753\n",
      "Epoch 214, Average Loss: 0.6558\n",
      "Epoch 215, Average Loss: 0.6626\n",
      "Epoch 216, Average Loss: 0.6518\n",
      "Epoch 217, Average Loss: 0.6541\n",
      "Epoch 218, Average Loss: 0.6540\n",
      "Epoch 219, Average Loss: 0.6516\n",
      "Epoch 220, Average Loss: 0.6526\n",
      "Epoch 221, Average Loss: 0.6532\n",
      "Epoch 222, Average Loss: 0.6455\n",
      "Epoch 223, Average Loss: 0.6572\n",
      "Epoch 224, Average Loss: 0.6476\n",
      "Epoch 225, Average Loss: 0.6708\n",
      "Epoch 226, Average Loss: 0.6759\n",
      "Epoch 227, Average Loss: 0.6460\n",
      "Epoch 228, Average Loss: 0.6421\n",
      "Epoch 229, Average Loss: 0.6445\n",
      "Epoch 230, Average Loss: 0.6677\n",
      "Epoch 231, Average Loss: 0.6558\n",
      "Epoch 232, Average Loss: 0.6613\n",
      "Epoch 233, Average Loss: 0.6484\n",
      "Epoch 234, Average Loss: 0.6546\n",
      "Epoch 235, Average Loss: 0.6730\n",
      "Epoch 236, Average Loss: 0.6402\n",
      "Epoch 237, Average Loss: 0.6403\n",
      "Epoch 238, Average Loss: 0.6505\n",
      "Epoch 239, Average Loss: 0.6517\n",
      "Epoch 240, Average Loss: 0.6529\n",
      "Epoch 241, Average Loss: 0.6526\n",
      "Epoch 242, Average Loss: 0.6421\n",
      "Epoch 243, Average Loss: 0.6309\n",
      "Epoch 244, Average Loss: 0.6455\n",
      "Epoch 245, Average Loss: 0.6427\n",
      "Epoch 246, Average Loss: 0.6352\n",
      "Epoch 247, Average Loss: 0.6302\n",
      "Epoch 248, Average Loss: 0.6336\n"
     ]
    }
   ],
   "source": [
    "class VectorField(nn.Module):\n",
    "    \"\"\"Vector field model based on DiT\"\"\"\n",
    "    def __init__(self, img_size=128, patch_size=32, in_channels=3, dim=512, depth=6, heads=8):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        \n",
    "        self.dit = MinimalDiT(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            dim=dim,\n",
    "            depth=depth,\n",
    "            heads=heads\n",
    "        )\n",
    "    \n",
    "    def forward(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Process time embedding\n",
    "        t_emb = self.time_mlp(t.unsqueeze(-1))\n",
    "        \n",
    "        # Add time embedding to each patch\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.dit.patch_embed(x)  # Convert to patches\n",
    "        x = x + t_emb.unsqueeze(1)   # Add time embedding to each patch\n",
    "        \n",
    "        # Process through transformer blocks\n",
    "        for block in self.dit.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        # Final processing\n",
    "        x = self.dit.norm(x)\n",
    "        x = self.dit.to_out(x)\n",
    "        x = self.dit.unpatchify(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_flow_matching(\n",
    "    dataloader,\n",
    "    num_epochs=1000,\n",
    "    batch_size=32,\n",
    "    device=\"cuda\",\n",
    "    sigma_min=0.001,\n",
    "    img_size=128,\n",
    "    patch_size=16,\n",
    "    dim=512,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    save_every=100\n",
    "):\n",
    "    # Initialize vector field model\n",
    "    model = VectorField(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        in_channels=3,\n",
    "        dim=dim,\n",
    "        depth=depth,\n",
    "        heads=heads\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamWScheduleFree(\n",
    "        model.parameters(),\n",
    "        lr=1e-4,\n",
    "        warmup_steps=1000,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    optimizer.train()\n",
    "    \n",
    "    # Training loop\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            x1 = batch[0]\n",
    "            batch_size = x1.shape[0]\n",
    "            \n",
    "            # Sample random times t ∈ [0,1]\n",
    "            t = torch.rand(batch_size, device=device)\n",
    "            \n",
    "            # Sample noise\n",
    "            x0 = torch.randn_like(x1)\n",
    "            \n",
    "            # Compute OT interpolation\n",
    "            sigma_t = 1 - (1 - sigma_min) * t.reshape(-1, 1, 1, 1)\n",
    "            mu_t = t.reshape(-1, 1, 1, 1) * x1\n",
    "            x_t = sigma_t * x0 + mu_t\n",
    "            \n",
    "            # Get vector field prediction\n",
    "            v_t = model(t, x_t)\n",
    "            \n",
    "            # Flow matching loss\n",
    "            target = x1 - (1 - sigma_min) * x0\n",
    "            loss = torch.mean((v_t - target) ** 2)\n",
    "            \n",
    "            # Optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint if loss improved\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, 'best_model.pt')\n",
    "        \n",
    "        # Regular checkpoint saving\n",
    "        if epoch % save_every == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, f'checkpoint_epoch_{epoch}.pt')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "config = {\n",
    "    'num_epochs': 1000,\n",
    "    'batch_size': batch_size,\n",
    "    'device': device,\n",
    "    'sigma_min': 0.001,\n",
    "    'img_size': img_size,\n",
    "    'patch_size': 16,\n",
    "    'dim': 512,\n",
    "    'depth': 6,\n",
    "    'heads': 8,\n",
    "    'save_every': 50000\n",
    "}\n",
    "\n",
    "# Start training\n",
    "model = train_flow_matching(dataloader, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb377e-dbb6-4400-b7e5-1aa68f830f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dit(\n",
    "    vector_field,\n",
    "    n_samples=16,\n",
    "    n_steps=50,\n",
    "    image_size=128,\n",
    "    device=\"cuda\",\n",
    "    sigma_min=0.001,\n",
    "    show_progress=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample images from a trained DiT vector field model\n",
    "    \n",
    "    Args:\n",
    "        vector_field: Trained VectorField model wrapping DiT\n",
    "        n_samples: Number of images to generate\n",
    "        n_steps: Number of integration steps\n",
    "        image_size: Size of generated images\n",
    "        device: Device to run sampling on\n",
    "        sigma_min: Minimum noise level\n",
    "        show_progress: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of generated images\n",
    "    \"\"\"\n",
    "    vector_field.eval()\n",
    "    \n",
    "    # Start from pure noise (t=0)\n",
    "    x = torch.randn(n_samples, 3, image_size, image_size, device=device)\n",
    "    \n",
    "    # Time steps from t=0 to t=1\n",
    "    ts = torch.linspace(0, 1, n_steps, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, t in enumerate(ts):\n",
    "            # Get vector field prediction\n",
    "            v = vector_field(t.expand(n_samples), x)\n",
    "            \n",
    "            # Euler integration step\n",
    "            x = x + v * (1/n_steps)\n",
    "            \n",
    "            if show_progress and (i + 1) % 10 == 0:\n",
    "                print(f\"Sampling step {i+1}/{n_steps}\")\n",
    "                \n",
    "    # Clamp values to valid image range [-1, 1]\n",
    "    x = torch.clamp(x, -1, 1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc0ed6-7792-4b59-8ca5-dee06c97d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample_dit(\n",
    "    model,\n",
    "    n_samples=16,  # Number of images to generate\n",
    "    n_steps=50,    # Number of integration steps\n",
    "    image_size=img_size, # Should match model's img_size\n",
    "    sigma_min = 0.1\n",
    ")\n",
    "\n",
    "# Convert to displayable format if needed\n",
    "samples = (samples + 1) * 0.5  # Scale from [-1, 1] to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d7fcd5-f2fd-49e7-96d1-944a2209d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "def display_samples_plotly(samples, rows=4, cols=4):\n",
    "    \"\"\"\n",
    "    Display samples in an interactive plotly grid\n",
    "    \n",
    "    Args:\n",
    "        samples: Tensor of shape [N, C, H, W] in range [-1, 1]\n",
    "        rows: Number of rows in the grid\n",
    "        cols: Number of columns in the grid\n",
    "    \"\"\"\n",
    "    # Ensure we're working with normalized values\n",
    "    samples = (samples + 1) * 0.5\n",
    "    samples = samples.clamp(0, 1)\n",
    "    \n",
    "    # Convert to numpy and transpose to correct format\n",
    "    samples = samples.cpu().numpy()\n",
    "    samples = np.transpose(samples, (0, 2, 3, 1))\n",
    "    \n",
    "    # Create subplot grid\n",
    "    fig = px.imshow(\n",
    "        make_grid(torch.from_numpy(np.transpose(samples, (0, 3, 1, 2))), \n",
    "                 nrow=rows).permute(1, 2, 0).cpu().numpy(),\n",
    "        title=\"Generated Samples\"\n",
    "    )\n",
    "    \n",
    "    # Update layout for better display\n",
    "    fig.update_layout(\n",
    "        width=800,\n",
    "        height=800,\n",
    "        showlegend=False,\n",
    "        margin=dict(l=0, r=0, t=30, b=0)\n",
    "    )\n",
    "    \n",
    "    # Remove axes\n",
    "    fig.update_xaxes(showticklabels=False, showgrid=False)\n",
    "    fig.update_yaxes(showticklabels=False, showgrid=False)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "def display_samples_grid(samples, rows=4, cols=4):\n",
    "    \"\"\"\n",
    "    Alternative display using torchvision's make_grid\n",
    "    \n",
    "    Args:\n",
    "        samples: Tensor of shape [N, C, H, W] in range [-1, 1]\n",
    "        rows: Number of rows in the grid\n",
    "        cols: Number of columns in the grid\n",
    "    \"\"\"\n",
    "    # Normalize to [0, 1]\n",
    "    samples = (samples + 1) * 0.5\n",
    "    samples = samples.clamp(0, 1)\n",
    "    \n",
    "    # Create grid\n",
    "    grid = make_grid(samples, nrow=cols, padding=2, normalize=False)\n",
    "    \n",
    "    # Convert to plotly figure for better interaction\n",
    "    fig = px.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        width=800,\n",
    "        height=800,\n",
    "        showlegend=False,\n",
    "        margin=dict(l=0, r=0, t=0, b=0)\n",
    "    )\n",
    "    \n",
    "    # Remove axes\n",
    "    fig.update_xaxes(showticklabels=False, showgrid=False)\n",
    "    fig.update_yaxes(showticklabels=False, showgrid=False)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "display_samples_grid(samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
